{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SArgW_Vq-uTh"
   },
   "source": [
    "# **Milestone 4**\n",
    "\n",
    "Descriptions - To be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlFM4hig-uTj"
   },
   "source": [
    "## Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "E3alYkjM-uTk"
   },
   "outputs": [],
   "source": [
    "# Import all necessary python packages\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10,6]\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "4JEBC9tZEZel"
   },
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "\n",
    "df = pd.read_csv('Walmart.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfrfDK0P-uT5"
   },
   "source": [
    "## <font color = 'blue'> 1. Inspecting the Dataset </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "x7YLk8-UQ5XO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6435 entries, 0 to 6434\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Store         6435 non-null   int64  \n",
      " 1   Date          6435 non-null   object \n",
      " 2   Weekly_Sales  6435 non-null   float64\n",
      " 3   Holiday_Flag  6435 non-null   int64  \n",
      " 4   Temperature   6435 non-null   float64\n",
      " 5   Fuel_Price    6435 non-null   float64\n",
      " 6   CPI           6435 non-null   float64\n",
      " 7   Unemployment  6435 non-null   float64\n",
      "dtypes: float64(5), int64(2), object(1)\n",
      "memory usage: 402.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Inspect dataset structure\n",
    "print('Dataset structure:')\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      "Store             int64\n",
      "Date             object\n",
      "Weekly_Sales    float64\n",
      "Holiday_Flag      int64\n",
      "Temperature     float64\n",
      "Fuel_Price      float64\n",
      "CPI             float64\n",
      "Unemployment    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print('\\nData types:')\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics:\n",
      "             Store  Weekly_Sales  Holiday_Flag  Temperature   Fuel_Price  \\\n",
      "count  6435.000000  6.435000e+03   6435.000000  6435.000000  6435.000000   \n",
      "mean     23.000000  1.046965e+06      0.069930    60.663782     3.358607   \n",
      "std      12.988182  5.643666e+05      0.255049    18.444933     0.459020   \n",
      "min       1.000000  2.099862e+05      0.000000    -2.060000     2.472000   \n",
      "25%      12.000000  5.533501e+05      0.000000    47.460000     2.933000   \n",
      "50%      23.000000  9.607460e+05      0.000000    62.670000     3.445000   \n",
      "75%      34.000000  1.420159e+06      0.000000    74.940000     3.735000   \n",
      "max      45.000000  3.818686e+06      1.000000   100.140000     4.468000   \n",
      "\n",
      "               CPI  Unemployment  \n",
      "count  6435.000000   6435.000000  \n",
      "mean    171.578394      7.999151  \n",
      "std      39.356712      1.875885  \n",
      "min     126.064000      3.879000  \n",
      "25%     131.735000      6.891000  \n",
      "50%     182.616521      7.874000  \n",
      "75%     212.743293      8.622000  \n",
      "max     227.232807     14.313000  \n"
     ]
    }
   ],
   "source": [
    "# Summarize numerical columns\n",
    "print('\\nSummary statistics:')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "Store           0\n",
      "Date            0\n",
      "Weekly_Sales    0\n",
      "Holiday_Flag    0\n",
      "Temperature     0\n",
      "Fuel_Price      0\n",
      "CPI             0\n",
      "Unemployment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print('\\nMissing values:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Potential outliers:\n",
      "Store: count    6435.000000\n",
      "mean       23.000000\n",
      "std        12.988182\n",
      "min         1.000000\n",
      "25%        12.000000\n",
      "50%        23.000000\n",
      "75%        34.000000\n",
      "99%        45.000000\n",
      "max        45.000000\n",
      "Name: Store, dtype: float64\n",
      "Weekly_Sales: count    6.435000e+03\n",
      "mean     1.046965e+06\n",
      "std      5.643666e+05\n",
      "min      2.099862e+05\n",
      "25%      5.533501e+05\n",
      "50%      9.607460e+05\n",
      "75%      1.420159e+06\n",
      "99%      2.404035e+06\n",
      "max      3.818686e+06\n",
      "Name: Weekly_Sales, dtype: float64\n",
      "Holiday_Flag: count    6435.000000\n",
      "mean        0.069930\n",
      "std         0.255049\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "99%         1.000000\n",
      "max         1.000000\n",
      "Name: Holiday_Flag, dtype: float64\n",
      "Temperature: count    6435.000000\n",
      "mean       60.663782\n",
      "std        18.444933\n",
      "min        -2.060000\n",
      "25%        47.460000\n",
      "50%        62.670000\n",
      "75%        74.940000\n",
      "99%        93.190000\n",
      "max       100.140000\n",
      "Name: Temperature, dtype: float64\n",
      "Fuel_Price: count    6435.000000\n",
      "mean        3.358607\n",
      "std         0.459020\n",
      "min         2.472000\n",
      "25%         2.933000\n",
      "50%         3.445000\n",
      "75%         3.735000\n",
      "99%         4.203000\n",
      "max         4.468000\n",
      "Name: Fuel_Price, dtype: float64\n",
      "CPI: count    6435.000000\n",
      "mean      171.578394\n",
      "std        39.356712\n",
      "min       126.064000\n",
      "25%       131.735000\n",
      "50%       182.616521\n",
      "75%       212.743293\n",
      "99%       225.470151\n",
      "max       227.232807\n",
      "Name: CPI, dtype: float64\n",
      "Unemployment: count    6435.000000\n",
      "mean        7.999151\n",
      "std         1.875885\n",
      "min         3.879000\n",
      "25%         6.891000\n",
      "50%         7.874000\n",
      "75%         8.622000\n",
      "99%        14.180000\n",
      "max        14.313000\n",
      "Name: Unemployment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Identify potential outliers\n",
    "print('\\nPotential outliers:')\n",
    "for col in df.columns:\n",
    "    if not df[col].dtype == 'object':\n",
    "        print(f'{col}: {df[col].describe(percentiles=[.25, .75, .99])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"19-02-2010\" doesn't match format \"%m-%d-%Y\", at position 2. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[56], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Reframing the columns\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Convert 'Date' to datetime format\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m df\u001B[38;5;241m.\u001B[39mDate \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_datetime\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Extract weekday, month, and year from 'Date'\u001B[39;00m\n\u001B[0;32m      7\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweekday\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mDate\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mweekday\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\git\\Walmart-Sales-Forecasting\\venv\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1108\u001B[0m, in \u001B[0;36mto_datetime\u001B[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001B[0m\n\u001B[0;32m   1106\u001B[0m             result \u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39mtz_localize(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, ABCSeries):\n\u001B[1;32m-> 1108\u001B[0m     cache_array \u001B[38;5;241m=\u001B[39m \u001B[43m_maybe_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_listlike\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1109\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m cache_array\u001B[38;5;241m.\u001B[39mempty:\n\u001B[0;32m   1110\u001B[0m         result \u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39mmap(cache_array)\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\git\\Walmart-Sales-Forecasting\\venv\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:254\u001B[0m, in \u001B[0;36m_maybe_cache\u001B[1;34m(arg, format, cache, convert_listlike)\u001B[0m\n\u001B[0;32m    252\u001B[0m unique_dates \u001B[38;5;241m=\u001B[39m unique(arg)\n\u001B[0;32m    253\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(unique_dates) \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(arg):\n\u001B[1;32m--> 254\u001B[0m     cache_dates \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_listlike\u001B[49m\u001B[43m(\u001B[49m\u001B[43munique_dates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    255\u001B[0m     \u001B[38;5;66;03m# GH#45319\u001B[39;00m\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\git\\Walmart-Sales-Forecasting\\venv\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:488\u001B[0m, in \u001B[0;36m_convert_listlike_datetimes\u001B[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001B[0m\n\u001B[0;32m    486\u001B[0m \u001B[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001B[39;00m\n\u001B[0;32m    487\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmixed\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_array_strptime_with_fallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mutc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexact\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    490\u001B[0m result, tz_parsed \u001B[38;5;241m=\u001B[39m objects_to_datetime64ns(\n\u001B[0;32m    491\u001B[0m     arg,\n\u001B[0;32m    492\u001B[0m     dayfirst\u001B[38;5;241m=\u001B[39mdayfirst,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    496\u001B[0m     allow_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    497\u001B[0m )\n\u001B[0;32m    499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tz_parsed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    500\u001B[0m     \u001B[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001B[39;00m\n\u001B[0;32m    501\u001B[0m     \u001B[38;5;66;03m# is in UTC\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Desktop\\git\\Walmart-Sales-Forecasting\\venv\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py:519\u001B[0m, in \u001B[0;36m_array_strptime_with_fallback\u001B[1;34m(arg, name, utc, fmt, exact, errors)\u001B[0m\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_array_strptime_with_fallback\u001B[39m(\n\u001B[0;32m    509\u001B[0m     arg,\n\u001B[0;32m    510\u001B[0m     name,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    514\u001B[0m     errors: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m    515\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Index:\n\u001B[0;32m    516\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001B[39;00m\n\u001B[0;32m    518\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 519\u001B[0m     result, timezones \u001B[38;5;241m=\u001B[39m \u001B[43marray_strptime\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfmt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexact\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexact\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mutc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mutc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(tz \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m tz \u001B[38;5;129;01min\u001B[39;00m timezones):\n\u001B[0;32m    521\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001B[1;32mstrptime.pyx:534\u001B[0m, in \u001B[0;36mpandas._libs.tslibs.strptime.array_strptime\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mstrptime.pyx:355\u001B[0m, in \u001B[0;36mpandas._libs.tslibs.strptime.array_strptime\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: time data \"19-02-2010\" doesn't match format \"%m-%d-%Y\", at position 2. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# Reframing the columns\n",
    "\n",
    "# Convert 'Date' to datetime format\n",
    "df.Date = pd.to_datetime(df.Date)\n",
    "\n",
    "# Extract weekday, month, and year from 'Date'\n",
    "df['weekday'] = df.Date.dt.weekday\n",
    "df['month'] = df.Date.dt.month\n",
    "df['year'] = df.Date.dt.year\n",
    "\n",
    "# Drop 'Date' column\n",
    "df.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Define target and feature columns\n",
    "target = 'Weekly_Sales'\n",
    "features = [col for col in df.columns if col != target]\n",
    "\n",
    "# Create a copy of the original dataframe\n",
    "original_df = df.copy(deep=True)\n",
    "\n",
    "# Display the first few rows of the reframed dataframe\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the dtypes of all the columns\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of unique rows in each feature\n",
    "\n",
    "df.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique rows in each feature\n",
    "unique_values = df[features].nunique()\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = unique_values[unique_values <= 45].index.tolist()\n",
    "categorical_features = unique_values[unique_values > 45].index.tolist()\n",
    "\n",
    "print('\\n\\033[1mInference:\\033[0m The Dataset has {} numerical & {} categorical features.'.format(len(numerical_features), len(categorical_features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the stats of all the columns\n",
    "\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us first analyze the distribution of the target variable\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the target variable\n",
    "target = 'Weekly_Sales'\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Kernel density estimation plot\n",
    "sns.kdeplot(df[target], color='blue', shade=True, label='Density')\n",
    "\n",
    "# Rugplot to show individual data points\n",
    "sns.rugplot(df[target], color='gray', alpha=0.5)\n",
    "\n",
    "# Quantile lines\n",
    "plt.axvline(df[target].quantile(0.25), color='green', linestyle='dashed', linewidth=2, label='Q1')\n",
    "plt.axvline(df[target].quantile(0.50), color='orange', linestyle='dashed', linewidth=2, label='Median')\n",
    "plt.axvline(df[target].quantile(0.75), color='red', linestyle='dashed', linewidth=2, label='Q3')\n",
    "\n",
    "# Logarithmic x-axis\n",
    "plt.xscale('log')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Weekly Sales ($)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Weekly Sales')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising the categorical features \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the list of categorical features\n",
    "categorical_features = df[features].columns[df[features].nunique() <= 45]\n",
    "\n",
    "# Define the number of rows and columns for subplots\n",
    "num_rows = math.ceil(len(categorical_features) / 3)\n",
    "num_cols = 3\n",
    "\n",
    "# Create the figure\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 15))\n",
    "\n",
    "# Iterate over categorical features and create subplots\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "\n",
    "    # Create the subplot\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Countplot for the categorical feature\n",
    "    sns.countplot(df[feature], ax=ax)\n",
    "\n",
    "    # Set subplot title and labels\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('Unique Values')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "# Adjust subplot layout and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get the list of numerical features\n",
    "numerical_features = df[features].columns[df[features].nunique() > 45]\n",
    "\n",
    "# Define the number of rows and columns for subplots\n",
    "num_rows = math.ceil(len(numerical_features) / 4)\n",
    "num_cols = 4\n",
    "\n",
    "# Color palette for distribution plots\n",
    "color_palette = sns.color_palette('hls', len(numerical_features))\n",
    "\n",
    "# Create a figure for distribution plots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15), squeeze=False)\n",
    "\n",
    "# Iterate over numerical features and create distribution plots\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "\n",
    "    # Create the subplot\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Distribution plot with adjusted bin size\n",
    "    sns.distplot(df[feature], ax=ax, hist_kws=dict(edgecolor=\"black\", linewidth=2), bins=20, color=color_palette[i])\n",
    "\n",
    "    # Set subplot title and labels\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "# Adjust subplot layout and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a figure for boxplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15), squeeze=False)\n",
    "\n",
    "# Iterate over numerical features and create boxplots\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "\n",
    "    # Create the subplot\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Boxplot with customized aesthetics\n",
    "    df.boxplot(feature, ax=ax)\n",
    "\n",
    "    # Set subplot title and labels\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "# Tight layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the relationship between all the features\n",
    "\n",
    "g = sns.pairplot(df)\n",
    "plt.title('Pairplots for all the Feature')\n",
    "g.map_upper(sns.kdeplot, levels=4, color=\".2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe to preserve the original data\n",
    "original_df = df.copy(deep=True)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates_exist = df.duplicated().any()\n",
    "\n",
    "# Remove duplicate rows if they exist\n",
    "if duplicates_exist:\n",
    "    print('\\n\\033[1mRemoving duplicate rows...')\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Calculate the number of duplicates removed\n",
    "    num_duplicates_removed = original_df.shape[0] - df.shape[0]\n",
    "\n",
    "    # Print a message indicating the number of duplicates removed\n",
    "    print(f'\\n\\033[1mNumber of duplicates removed/fixed: {num_duplicates_removed}')\n",
    "\n",
    "# If there are no duplicates, print an informative message\n",
    "else:\n",
    "    print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for empty elements\n",
    "\n",
    "nvc = pd.DataFrame(df.isnull().sum().sort_values(), columns=['Total Null Values'])\n",
    "nvc['Percentage'] = round(nvc['Total Null Values']/df.shape[0],3)*100\n",
    "print(nvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty elements\n",
    "missing_values = df.isnull().sum().reset_index().rename(columns={'index': 'Feature', 0: 'Total Missing Values'})\n",
    "missing_values['Percentage Missing Values'] = round(missing_values['Total Missing Values'] / df.shape[0], 3) * 100\n",
    "\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe\n",
    "df3 = df.copy()\n",
    "\n",
    "# Identify categorical columns with non-zero missing values\n",
    "non_zero_missing_categorical_columns = nvc[nvc['Percentage'] != 0].index.tolist()\n",
    "\n",
    "# Separate columns for one-hot encoding and dummy encoding\n",
    "one_hot_columns = [i for i in categorical_features if i in non_zero_missing_categorical_columns and df3[i].nunique() == 2]\n",
    "dummy_columns = [i for i in categorical_features if i in non_zero_missing_categorical_columns and df3[i].nunique() > 2]\n",
    "\n",
    "# Perform one-hot encoding for columns with two unique values\n",
    "if len(one_hot_columns) > 0:\n",
    "    print(\"\\n\\033[1mOne-Hot Encoding on features:\")\n",
    "    for col in one_hot_columns:\n",
    "        print(col)\n",
    "        df3[col] = pd.get_dummies(df3[col], drop_first=True, prefix=col)\n",
    "\n",
    "# Perform dummy encoding for columns with more than two unique values\n",
    "if len(dummy_columns) > 0:\n",
    "    print(\"\\n\\033[1mDummy Encoding on features:\")\n",
    "    for col in dummy_columns:\n",
    "        print(col)\n",
    "        df3 = pd.concat([df3.drop([col], axis=1), pd.DataFrame(pd.get_dummies(df3[col], drop_first=True, prefix=col))], axis=1)\n",
    "\n",
    "print(df3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMEcdAp3-uT-"
   },
   "source": [
    "##  <font color = 'blue'> 2. IQR Outlier Removal </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe\n",
    "df1 = df3.copy()\n",
    "\n",
    "# Define features for outlier removal\n",
    "features1 = numerical_features\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "for col in features1:\n",
    "    # Calculate IQR more efficiently\n",
    "    q1 = df1[col].quantile(0.25)\n",
    "    q3 = df1[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # Filter data within 1.5 IQR of quartiles\n",
    "    df1 = df1[df1[col] <= (q3 + 1.5 * iqr)]\n",
    "    df1 = df1[df1[col] >= (q1 - 1.5 * iqr)]\n",
    "\n",
    "    # Reset index after each filtering\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "\n",
    "# Display the head of the processed dataset\n",
    "display(df1.head())\n",
    "\n",
    "# Print informative messages about outlier removal\n",
    "print(\"\\n\\033[1mInference:\\033[0m\\nBefore outlier removal, the dataset had {} samples.\".format(df3.shape[0]))\n",
    "print(\"After outlier removal, the dataset now has {} samples.\".format(df1.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMEcdAp3-uT-"
   },
   "source": [
    "##  <font color = 'blue'> 3. PCA feature elimination </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df1 is the dataframe after outlier removal\n",
    "# Extract features and target variable\n",
    "X = df1.drop(target, axis=1)\n",
    "y = df1[target]\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Task 1: Code Review\n",
    "# Creating and fitting the PCA object\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Task 2: Exploration of Explained Variance\n",
    "# Plot the explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Task 3: Threshold Adjustment\n",
    "# Evaluate the explained variance ratio for each principal component\n",
    "explained_variance_ratios = pca.explained_variance_ratio_\n",
    "\n",
    "# Choose a threshold, e.g., 0.95 for retaining 95% of the variance\n",
    "threshold = 0.95\n",
    "cumulative_variance = 0\n",
    "num_components_to_keep = 0\n",
    "\n",
    "for i, explained_variance_ratio in enumerate(explained_variance_ratios):\n",
    "    cumulative_variance += explained_variance_ratio\n",
    "    if cumulative_variance >= threshold:\n",
    "        num_components_to_keep = i + 1\n",
    "        break\n",
    "\n",
    "print(f\"Number of components to retain {threshold * 100}% variance: {num_components_to_keep}\")\n",
    "\n",
    "# Retain only the selected number of components\n",
    "pca = PCA(n_components=num_components_to_keep)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Task 4: Alternative Approaches\n",
    "# Research and propose alternative methods for feature elimination\n",
    "# (e.g., Recursive Feature Elimination, L1 regularization, etc.)\n",
    "\n",
    "# Example of Recursive Feature Elimination (RFE)\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create the RFE model and select the number of features to retain\n",
    "num_features_to_retain = 5\n",
    "rfe = RFE(model, num_features_to_retain)\n",
    "X_rfe = rfe.fit_transform(X_scaled, y)\n",
    "\n",
    "# Display the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(\"Selected features using RFE:\", selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMEcdAp3-uT-"
   },
   "source": [
    "##  <font color = 'blue'> Data Frame for Hamid </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the selected principal components\n",
    "df_pca = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(num_components_to_keep)])\n",
    "\n",
    "# Provide df_pca to the Hamid for building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print explained variance ratio for each principal component\n",
    "explained_variance_ratios = pca.explained_variance_ratio_\n",
    "for i, explained_variance_ratio in enumerate(explained_variance_ratios):\n",
    "    print(f\"Explained Variance Ratio for PC{i+1}: {explained_variance_ratio:.3f}\")\n",
    "\n",
    "# Print contributions of original features to each principal component\n",
    "component_names = [f'PC{i+1}' for i in range(num_components_to_keep)]\n",
    "components_df = pd.DataFrame(pca.components_, columns=X.columns, index=component_names)\n",
    "\n",
    "# Display the DataFrame with feature contributions to each principal component\n",
    "print(\"\\nContributions of Original Features to Principal Components:\")\n",
    "print(components_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of Principal Component Analysis (PCA) provide valuable insights into the data reduction process. \n",
    "- The explained variance ratios for each principal component (PC) indicate the proportion of the total variance captured by that component. \n",
    " - For example, PC1 explains approximately 21.5% of the total variance, followed by PC2 with 16.6%, and so on. - Examining the contributions of the original features to each PC reveals the key factors influencing the components. \n",
    "  - Notably, PC1 is influenced by factors like 'Holiday_Flag,' 'Unemployment,' and 'year,' while PC2 shows strong negative contributions from 'Store,' 'CPI,' and 'Unemployment.' \n",
    "  - Understanding these contributions aids in comprehending the significance of each PC and guides feature selection for subsequent modeling tasks. \n",
    " - It's important to note that the cumulative explained variance provides a holistic view of the data's information retention, assisting in determining the optimal number of principal components to retain for modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[59], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[43mdf1\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWeekly_Sales\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      2\u001B[0m X \u001B[38;5;241m=\u001B[39m df1\u001B[38;5;241m.\u001B[39miloc[:,\u001B[38;5;241m2\u001B[39m:]\n\u001B[0;32m      3\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X,y,test_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m, random_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "y = df1['Weekly_Sales']\n",
    "X = df1.iloc[:,2:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 3)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train,y_train)\n",
    "print(f\"Training r^2: {linear_model.score(X_train,y_train):.4f}\")\n",
    "print(f\"Test r^2: {linear_model.score(X_test,y_test):.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train,y_train)\n",
    "print(f\"Training r^2: {rf_model.score(X_train,y_train):.4f}\")\n",
    "print(f\"Test r^2: {rf_model.score(X_test,y_test):.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_model = Lasso()\n",
    "linear_model.fit(X_train,y_train)\n",
    "print(f\"Training r^2: {linear_model.score(X_train,y_train):.4f}\")\n",
    "print(f\"Test r^2: {linear_model.score(X_test,y_test):.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# random forest hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\"n_estimators\":[50,100,200],\n",
    "             \"max_depth\":[2,5,8],\n",
    "             \"min_samples_split\":[4,8,12]}\n",
    "rf_cv = GridSearchCV(rf_model, param_grid = param_grid, verbose = 1)\n",
    "rf_cv.fit(X_train, y_train)\n",
    "print(rf_cv.best_params_)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "gb_model.fit(X_train,y_train)\n",
    "print(f\"Training r^2: {gb_model.score(X_train,y_train):.4f}\")\n",
    "print(f\"Test r^2: {gb_model.score(X_test,y_test):.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
